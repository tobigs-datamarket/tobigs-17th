{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu39oBW0RVn5"
   },
   "source": [
    "# [과제 3] 로지스틱 회귀분석\n",
    "### - sklearn 패키지를 사용해 로지스틱 회귀분석을 진행해주세요.\n",
    "### - 성능지표를 계산하고 이에 대해 해석해주세요.\n",
    "### - 성능 개선을 시도해주세요. (어떠한 성능지표를 기준으로 개선을 시도했는지, 그 이유도 함께 적어주세요.)\n",
    "### - 주석으로 설명 및 근거 자세하게 달아주시면 감사하겠습니다. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rN2SWezRVn_"
   },
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7SYKNvQRVn_"
   },
   "source": [
    "출처 : https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "\n",
    "* V1 ~ V28 : 비식별화 된 개인정보 \n",
    "* **Class** : Target 변수  \n",
    "  - 1 : fraudulent transactions (사기)\n",
    "  - 0 : otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uvjw2fTCRVoA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "znQit70ZRVoA"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"assignment3_creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "v98OeXW5RVoB",
    "outputId": "42afeddc-07e6-4224-95ee-08b455f72475",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.848212</td>\n",
       "      <td>2.384900</td>\n",
       "      <td>0.379573</td>\n",
       "      <td>1.048381</td>\n",
       "      <td>-0.845070</td>\n",
       "      <td>2.537837</td>\n",
       "      <td>-4.542983</td>\n",
       "      <td>-10.201458</td>\n",
       "      <td>-1.504967</td>\n",
       "      <td>-2.234167</td>\n",
       "      <td>...</td>\n",
       "      <td>2.585817</td>\n",
       "      <td>-5.291690</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.423231</td>\n",
       "      <td>-0.506985</td>\n",
       "      <td>1.020052</td>\n",
       "      <td>-0.627751</td>\n",
       "      <td>-0.017753</td>\n",
       "      <td>0.280982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.071805</td>\n",
       "      <td>-0.477943</td>\n",
       "      <td>-1.444444</td>\n",
       "      <td>-0.548657</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>-0.582242</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.247160</td>\n",
       "      <td>1.171923</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077306</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>0.390125</td>\n",
       "      <td>0.041569</td>\n",
       "      <td>0.598427</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>0.979686</td>\n",
       "      <td>-0.093244</td>\n",
       "      <td>-0.065615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.985294</td>\n",
       "      <td>-2.747472</td>\n",
       "      <td>1.194068</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>-1.151041</td>\n",
       "      <td>-0.263559</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.438545</td>\n",
       "      <td>-1.806488</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345776</td>\n",
       "      <td>0.373760</td>\n",
       "      <td>-0.385777</td>\n",
       "      <td>1.197596</td>\n",
       "      <td>0.407229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.762362</td>\n",
       "      <td>-0.299024</td>\n",
       "      <td>-0.303929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.479452</td>\n",
       "      <td>1.542874</td>\n",
       "      <td>0.290895</td>\n",
       "      <td>0.838142</td>\n",
       "      <td>-0.529290</td>\n",
       "      <td>-0.717661</td>\n",
       "      <td>0.484516</td>\n",
       "      <td>0.545092</td>\n",
       "      <td>-0.780767</td>\n",
       "      <td>0.324804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038397</td>\n",
       "      <td>0.116771</td>\n",
       "      <td>0.405560</td>\n",
       "      <td>-0.116453</td>\n",
       "      <td>0.541275</td>\n",
       "      <td>-0.216665</td>\n",
       "      <td>-0.415578</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>-0.150347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.281976</td>\n",
       "      <td>-0.309699</td>\n",
       "      <td>-2.162299</td>\n",
       "      <td>-0.851514</td>\n",
       "      <td>0.106167</td>\n",
       "      <td>-1.483888</td>\n",
       "      <td>1.930994</td>\n",
       "      <td>-0.843049</td>\n",
       "      <td>-1.249272</td>\n",
       "      <td>1.079608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875516</td>\n",
       "      <td>-0.004199</td>\n",
       "      <td>1.015108</td>\n",
       "      <td>-0.026748</td>\n",
       "      <td>0.077115</td>\n",
       "      <td>-1.468822</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.331001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.848212  2.384900  0.379573  1.048381 -0.845070  2.537837 -4.542983   \n",
       "1  2.071805 -0.477943 -1.444444 -0.548657  0.010036 -0.582242 -0.042878   \n",
       "2 -2.985294 -2.747472  1.194068 -0.003036 -1.151041 -0.263559  0.553500   \n",
       "3 -1.479452  1.542874  0.290895  0.838142 -0.529290 -0.717661  0.484516   \n",
       "4 -0.281976 -0.309699 -2.162299 -0.851514  0.106167 -1.483888  1.930994   \n",
       "\n",
       "          V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0 -10.201458 -1.504967 -2.234167  ...  2.585817 -5.291690  0.859364  0.423231   \n",
       "1  -0.247160  1.171923 -0.342382  ... -0.077306  0.042858  0.390125  0.041569   \n",
       "2   0.635600  0.438545 -1.806488  ...  1.345776  0.373760 -0.385777  1.197596   \n",
       "3   0.545092 -0.780767  0.324804  ...  0.038397  0.116771  0.405560 -0.116453   \n",
       "4  -0.843049 -1.249272  1.079608  ... -0.875516 -0.004199  1.015108 -0.026748   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0 -0.506985  1.020052 -0.627751 -0.017753  0.280982      0  \n",
       "1  0.598427  0.098803  0.979686 -0.093244 -0.065615      0  \n",
       "2  0.407229  0.008013  0.762362 -0.299024 -0.303929      0  \n",
       "3  0.541275 -0.216665 -0.415578  0.027126 -0.150347      0  \n",
       "4  0.077115 -1.468822  0.751700  0.496732  0.331001      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.848212</td>\n",
       "      <td>2.384900</td>\n",
       "      <td>0.379573</td>\n",
       "      <td>1.048381</td>\n",
       "      <td>-0.845070</td>\n",
       "      <td>2.537837</td>\n",
       "      <td>-4.542983</td>\n",
       "      <td>-10.201458</td>\n",
       "      <td>-1.504967</td>\n",
       "      <td>-2.234167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159752</td>\n",
       "      <td>2.585817</td>\n",
       "      <td>-5.291690</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.423231</td>\n",
       "      <td>-0.506985</td>\n",
       "      <td>1.020052</td>\n",
       "      <td>-0.627751</td>\n",
       "      <td>-0.017753</td>\n",
       "      <td>0.280982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.071805</td>\n",
       "      <td>-0.477943</td>\n",
       "      <td>-1.444444</td>\n",
       "      <td>-0.548657</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>-0.582242</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.247160</td>\n",
       "      <td>1.171923</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644970</td>\n",
       "      <td>-0.077306</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>0.390125</td>\n",
       "      <td>0.041569</td>\n",
       "      <td>0.598427</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>0.979686</td>\n",
       "      <td>-0.093244</td>\n",
       "      <td>-0.065615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.985294</td>\n",
       "      <td>-2.747472</td>\n",
       "      <td>1.194068</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>-1.151041</td>\n",
       "      <td>-0.263559</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.438545</td>\n",
       "      <td>-1.806488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.885471</td>\n",
       "      <td>1.345776</td>\n",
       "      <td>0.373760</td>\n",
       "      <td>-0.385777</td>\n",
       "      <td>1.197596</td>\n",
       "      <td>0.407229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.762362</td>\n",
       "      <td>-0.299024</td>\n",
       "      <td>-0.303929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.479452</td>\n",
       "      <td>1.542874</td>\n",
       "      <td>0.290895</td>\n",
       "      <td>0.838142</td>\n",
       "      <td>-0.529290</td>\n",
       "      <td>-0.717661</td>\n",
       "      <td>0.484516</td>\n",
       "      <td>0.545092</td>\n",
       "      <td>-0.780767</td>\n",
       "      <td>0.324804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663398</td>\n",
       "      <td>0.038397</td>\n",
       "      <td>0.116771</td>\n",
       "      <td>0.405560</td>\n",
       "      <td>-0.116453</td>\n",
       "      <td>0.541275</td>\n",
       "      <td>-0.216665</td>\n",
       "      <td>-0.415578</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>-0.150347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.281976</td>\n",
       "      <td>-0.309699</td>\n",
       "      <td>-2.162299</td>\n",
       "      <td>-0.851514</td>\n",
       "      <td>0.106167</td>\n",
       "      <td>-1.483888</td>\n",
       "      <td>1.930994</td>\n",
       "      <td>-0.843049</td>\n",
       "      <td>-1.249272</td>\n",
       "      <td>1.079608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531789</td>\n",
       "      <td>-0.875516</td>\n",
       "      <td>-0.004199</td>\n",
       "      <td>1.015108</td>\n",
       "      <td>-0.026748</td>\n",
       "      <td>0.077115</td>\n",
       "      <td>-1.468822</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.331001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28673</th>\n",
       "      <td>2.063299</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>-1.042161</td>\n",
       "      <td>0.409655</td>\n",
       "      <td>-0.069835</td>\n",
       "      <td>-1.198490</td>\n",
       "      <td>0.243507</td>\n",
       "      <td>-0.385099</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.047861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137028</td>\n",
       "      <td>-0.169749</td>\n",
       "      <td>-0.278942</td>\n",
       "      <td>-0.625629</td>\n",
       "      <td>0.331276</td>\n",
       "      <td>0.070205</td>\n",
       "      <td>-0.269826</td>\n",
       "      <td>0.192509</td>\n",
       "      <td>-0.064914</td>\n",
       "      <td>-0.058058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28674</th>\n",
       "      <td>1.264844</td>\n",
       "      <td>-1.228616</td>\n",
       "      <td>1.579098</td>\n",
       "      <td>-0.204514</td>\n",
       "      <td>-1.958881</td>\n",
       "      <td>0.546495</td>\n",
       "      <td>-1.764072</td>\n",
       "      <td>0.470377</td>\n",
       "      <td>1.002209</td>\n",
       "      <td>0.274005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643614</td>\n",
       "      <td>-0.125401</td>\n",
       "      <td>0.144909</td>\n",
       "      <td>0.718615</td>\n",
       "      <td>-0.038493</td>\n",
       "      <td>0.103706</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>-0.009807</td>\n",
       "      <td>0.089794</td>\n",
       "      <td>0.019848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28675</th>\n",
       "      <td>2.072659</td>\n",
       "      <td>0.190048</td>\n",
       "      <td>-1.759688</td>\n",
       "      <td>0.378909</td>\n",
       "      <td>0.556826</td>\n",
       "      <td>-0.769874</td>\n",
       "      <td>0.246899</td>\n",
       "      <td>-0.274561</td>\n",
       "      <td>0.380960</td>\n",
       "      <td>-0.405147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104002</td>\n",
       "      <td>-0.084170</td>\n",
       "      <td>-0.361188</td>\n",
       "      <td>-0.910782</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>0.401511</td>\n",
       "      <td>-0.212440</td>\n",
       "      <td>0.177857</td>\n",
       "      <td>-0.058961</td>\n",
       "      <td>-0.029035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28676</th>\n",
       "      <td>-0.690637</td>\n",
       "      <td>1.342271</td>\n",
       "      <td>1.498129</td>\n",
       "      <td>2.049774</td>\n",
       "      <td>1.055691</td>\n",
       "      <td>0.677197</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>-0.467242</td>\n",
       "      <td>-0.681576</td>\n",
       "      <td>1.485542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057182</td>\n",
       "      <td>0.030868</td>\n",
       "      <td>0.110939</td>\n",
       "      <td>0.453065</td>\n",
       "      <td>-0.355346</td>\n",
       "      <td>-0.771072</td>\n",
       "      <td>-0.358505</td>\n",
       "      <td>0.101309</td>\n",
       "      <td>-0.849084</td>\n",
       "      <td>-0.312999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28677</th>\n",
       "      <td>2.215783</td>\n",
       "      <td>-0.802708</td>\n",
       "      <td>-1.639556</td>\n",
       "      <td>-1.114807</td>\n",
       "      <td>-0.132806</td>\n",
       "      <td>-0.385132</td>\n",
       "      <td>-0.474623</td>\n",
       "      <td>-0.051786</td>\n",
       "      <td>-0.522953</td>\n",
       "      <td>0.989170</td>\n",
       "      <td>...</td>\n",
       "      <td>1.211906</td>\n",
       "      <td>-0.103507</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>0.195771</td>\n",
       "      <td>0.263594</td>\n",
       "      <td>-0.030135</td>\n",
       "      <td>-0.298560</td>\n",
       "      <td>-0.053755</td>\n",
       "      <td>-0.074604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28678 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     -1.848212  2.384900  0.379573  1.048381 -0.845070  2.537837 -4.542983   \n",
       "1      2.071805 -0.477943 -1.444444 -0.548657  0.010036 -0.582242 -0.042878   \n",
       "2     -2.985294 -2.747472  1.194068 -0.003036 -1.151041 -0.263559  0.553500   \n",
       "3     -1.479452  1.542874  0.290895  0.838142 -0.529290 -0.717661  0.484516   \n",
       "4     -0.281976 -0.309699 -2.162299 -0.851514  0.106167 -1.483888  1.930994   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28673  2.063299  0.015015 -1.042161  0.409655 -0.069835 -1.198490  0.243507   \n",
       "28674  1.264844 -1.228616  1.579098 -0.204514 -1.958881  0.546495 -1.764072   \n",
       "28675  2.072659  0.190048 -1.759688  0.378909  0.556826 -0.769874  0.246899   \n",
       "28676 -0.690637  1.342271  1.498129  2.049774  1.055691  0.677197  0.855445   \n",
       "28677  2.215783 -0.802708 -1.639556 -1.114807 -0.132806 -0.385132 -0.474623   \n",
       "\n",
       "              V8        V9       V10  ...       V19       V20       V21  \\\n",
       "0     -10.201458 -1.504967 -2.234167  ...  0.159752  2.585817 -5.291690   \n",
       "1      -0.247160  1.171923 -0.342382  ...  0.644970 -0.077306  0.042858   \n",
       "2       0.635600  0.438545 -1.806488  ... -0.885471  1.345776  0.373760   \n",
       "3       0.545092 -0.780767  0.324804  ...  0.663398  0.038397  0.116771   \n",
       "4      -0.843049 -1.249272  1.079608  ... -0.531789 -0.875516 -0.004199   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "28673  -0.385099  0.408691  0.047861  ...  0.137028 -0.169749 -0.278942   \n",
       "28674   0.470377  1.002209  0.274005  ...  0.643614 -0.125401  0.144909   \n",
       "28675  -0.274561  0.380960 -0.405147  ...  0.104002 -0.084170 -0.361188   \n",
       "28676  -0.467242 -0.681576  1.485542  ...  0.057182  0.030868  0.110939   \n",
       "28677  -0.051786 -0.522953  0.989170  ...  1.211906 -0.103507  0.034129   \n",
       "\n",
       "            V22       V23       V24       V25       V26       V27       V28  \n",
       "0      0.859364  0.423231 -0.506985  1.020052 -0.627751 -0.017753  0.280982  \n",
       "1      0.390125  0.041569  0.598427  0.098803  0.979686 -0.093244 -0.065615  \n",
       "2     -0.385777  1.197596  0.407229  0.008013  0.762362 -0.299024 -0.303929  \n",
       "3      0.405560 -0.116453  0.541275 -0.216665 -0.415578  0.027126 -0.150347  \n",
       "4      1.015108 -0.026748  0.077115 -1.468822  0.751700  0.496732  0.331001  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "28673 -0.625629  0.331276  0.070205 -0.269826  0.192509 -0.064914 -0.058058  \n",
       "28674  0.718615 -0.038493  0.103706  0.341100 -0.009807  0.089794  0.019848  \n",
       "28675 -0.910782  0.304372  0.401511 -0.212440  0.177857 -0.058961 -0.029035  \n",
       "28676  0.453065 -0.355346 -0.771072 -0.358505  0.101309 -0.849084 -0.312999  \n",
       "28677  0.022327  0.195771  0.263594 -0.030135 -0.298560 -0.053755 -0.074604  \n",
       "\n",
       "[28678 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=data.drop([\"Class\"],axis=1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90218019,  1.44246214,  0.24207645, ..., -1.29584288,\n",
       "        -0.04709654,  0.9417547 ],\n",
       "       [ 1.02744235, -0.32892601, -0.7953942 , ...,  2.02637926,\n",
       "        -0.23721911, -0.22629752],\n",
       "       [-1.46190736, -1.73320058,  0.7053479 , ...,  1.57721728,\n",
       "        -0.7554671 , -1.02943014],\n",
       "       ...,\n",
       "       [ 1.02786291,  0.0843943 , -0.9747001 , ...,  0.36917223,\n",
       "        -0.15087861, -0.1030219 ],\n",
       "       [-0.33236542,  0.79733419,  0.8782926 , ...,  0.21096583,\n",
       "        -2.140772  , -1.05999649],\n",
       "       [ 1.09831537, -0.52987507, -0.90637085, ..., -0.61547609,\n",
       "        -0.13776702, -0.2565925 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  scaling 진행합니다.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() # Scaling\n",
    "scaler.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(scaler.fit_transform(data2), columns = data2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"Class\"]=data.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.902180</td>\n",
       "      <td>1.442462</td>\n",
       "      <td>0.242076</td>\n",
       "      <td>0.682170</td>\n",
       "      <td>-0.545636</td>\n",
       "      <td>1.897453</td>\n",
       "      <td>-3.087657</td>\n",
       "      <td>-7.481774</td>\n",
       "      <td>-1.310739</td>\n",
       "      <td>-1.750813</td>\n",
       "      <td>...</td>\n",
       "      <td>3.586292</td>\n",
       "      <td>-6.258702</td>\n",
       "      <td>1.163043</td>\n",
       "      <td>0.721013</td>\n",
       "      <td>-0.838813</td>\n",
       "      <td>1.973207</td>\n",
       "      <td>-1.295843</td>\n",
       "      <td>-0.047097</td>\n",
       "      <td>0.941755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.027442</td>\n",
       "      <td>-0.328926</td>\n",
       "      <td>-0.795394</td>\n",
       "      <td>-0.395393</td>\n",
       "      <td>0.029429</td>\n",
       "      <td>-0.432294</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>-0.185605</td>\n",
       "      <td>1.049741</td>\n",
       "      <td>-0.240491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110982</td>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.528470</td>\n",
       "      <td>0.078108</td>\n",
       "      <td>0.993348</td>\n",
       "      <td>0.194594</td>\n",
       "      <td>2.026379</td>\n",
       "      <td>-0.237219</td>\n",
       "      <td>-0.226298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.461907</td>\n",
       "      <td>-1.733201</td>\n",
       "      <td>0.705348</td>\n",
       "      <td>-0.027248</td>\n",
       "      <td>-0.751404</td>\n",
       "      <td>-0.194335</td>\n",
       "      <td>0.415558</td>\n",
       "      <td>0.461429</td>\n",
       "      <td>0.403048</td>\n",
       "      <td>-1.409372</td>\n",
       "      <td>...</td>\n",
       "      <td>1.864714</td>\n",
       "      <td>0.429058</td>\n",
       "      <td>-0.520817</td>\n",
       "      <td>2.025422</td>\n",
       "      <td>0.676446</td>\n",
       "      <td>0.019311</td>\n",
       "      <td>1.577217</td>\n",
       "      <td>-0.755467</td>\n",
       "      <td>-1.029430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.720659</td>\n",
       "      <td>0.921458</td>\n",
       "      <td>0.191638</td>\n",
       "      <td>0.540317</td>\n",
       "      <td>-0.333272</td>\n",
       "      <td>-0.533410</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.395090</td>\n",
       "      <td>-0.672139</td>\n",
       "      <td>0.292162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049651</td>\n",
       "      <td>0.125696</td>\n",
       "      <td>0.549344</td>\n",
       "      <td>-0.188079</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>-0.414465</td>\n",
       "      <td>-0.857327</td>\n",
       "      <td>0.065929</td>\n",
       "      <td>-0.511850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.131203</td>\n",
       "      <td>-0.224824</td>\n",
       "      <td>-1.203698</td>\n",
       "      <td>-0.599739</td>\n",
       "      <td>0.094078</td>\n",
       "      <td>-1.105548</td>\n",
       "      <td>1.362418</td>\n",
       "      <td>-0.622372</td>\n",
       "      <td>-1.085267</td>\n",
       "      <td>0.894765</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.219154</td>\n",
       "      <td>-0.017103</td>\n",
       "      <td>1.373663</td>\n",
       "      <td>-0.036971</td>\n",
       "      <td>0.129301</td>\n",
       "      <td>-2.831946</td>\n",
       "      <td>1.555181</td>\n",
       "      <td>1.248615</td>\n",
       "      <td>1.110322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28673</th>\n",
       "      <td>1.023255</td>\n",
       "      <td>-0.023908</td>\n",
       "      <td>-0.566583</td>\n",
       "      <td>0.251205</td>\n",
       "      <td>-0.024285</td>\n",
       "      <td>-0.892442</td>\n",
       "      <td>0.202475</td>\n",
       "      <td>-0.286709</td>\n",
       "      <td>0.376723</td>\n",
       "      <td>0.071062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239323</td>\n",
       "      <td>-0.341422</td>\n",
       "      <td>-0.845179</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.117848</td>\n",
       "      <td>-0.517101</td>\n",
       "      <td>0.399455</td>\n",
       "      <td>-0.165871</td>\n",
       "      <td>-0.200830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28674</th>\n",
       "      <td>0.630217</td>\n",
       "      <td>-0.793406</td>\n",
       "      <td>0.924346</td>\n",
       "      <td>-0.163191</td>\n",
       "      <td>-1.294682</td>\n",
       "      <td>0.410528</td>\n",
       "      <td>-1.177492</td>\n",
       "      <td>0.340326</td>\n",
       "      <td>0.900087</td>\n",
       "      <td>0.251606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177753</td>\n",
       "      <td>0.158912</td>\n",
       "      <td>0.972702</td>\n",
       "      <td>-0.056755</td>\n",
       "      <td>0.173374</td>\n",
       "      <td>0.662386</td>\n",
       "      <td>-0.018687</td>\n",
       "      <td>0.223756</td>\n",
       "      <td>0.061716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28675</th>\n",
       "      <td>1.027863</td>\n",
       "      <td>0.084394</td>\n",
       "      <td>-0.974700</td>\n",
       "      <td>0.230460</td>\n",
       "      <td>0.397149</td>\n",
       "      <td>-0.572397</td>\n",
       "      <td>0.204807</td>\n",
       "      <td>-0.205689</td>\n",
       "      <td>0.352270</td>\n",
       "      <td>-0.290601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120511</td>\n",
       "      <td>-0.438509</td>\n",
       "      <td>-1.230805</td>\n",
       "      <td>0.520796</td>\n",
       "      <td>0.666969</td>\n",
       "      <td>-0.406308</td>\n",
       "      <td>0.369172</td>\n",
       "      <td>-0.150879</td>\n",
       "      <td>-0.103022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28676</th>\n",
       "      <td>-0.332365</td>\n",
       "      <td>0.797334</td>\n",
       "      <td>0.878293</td>\n",
       "      <td>1.357837</td>\n",
       "      <td>0.732640</td>\n",
       "      <td>0.508122</td>\n",
       "      <td>0.623108</td>\n",
       "      <td>-0.346918</td>\n",
       "      <td>-0.584673</td>\n",
       "      <td>1.218846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039199</td>\n",
       "      <td>0.118811</td>\n",
       "      <td>0.613586</td>\n",
       "      <td>-0.590491</td>\n",
       "      <td>-1.276522</td>\n",
       "      <td>-0.688309</td>\n",
       "      <td>0.210966</td>\n",
       "      <td>-2.140772</td>\n",
       "      <td>-1.059996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28677</th>\n",
       "      <td>1.098315</td>\n",
       "      <td>-0.529875</td>\n",
       "      <td>-0.906371</td>\n",
       "      <td>-0.777389</td>\n",
       "      <td>-0.066633</td>\n",
       "      <td>-0.285113</td>\n",
       "      <td>-0.291152</td>\n",
       "      <td>-0.042402</td>\n",
       "      <td>-0.444799</td>\n",
       "      <td>0.822563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147357</td>\n",
       "      <td>0.028141</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.337860</td>\n",
       "      <td>0.438380</td>\n",
       "      <td>-0.054341</td>\n",
       "      <td>-0.615476</td>\n",
       "      <td>-0.137767</td>\n",
       "      <td>-0.256593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28678 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     -0.902180  1.442462  0.242076  0.682170 -0.545636  1.897453 -3.087657   \n",
       "1      1.027442 -0.328926 -0.795394 -0.395393  0.029429 -0.432294  0.005620   \n",
       "2     -1.461907 -1.733201  0.705348 -0.027248 -0.751404 -0.194335  0.415558   \n",
       "3     -0.720659  0.921458  0.191638  0.540317 -0.333272 -0.533410  0.368140   \n",
       "4     -0.131203 -0.224824 -1.203698 -0.599739  0.094078 -1.105548  1.362418   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28673  1.023255 -0.023908 -0.566583  0.251205 -0.024285 -0.892442  0.202475   \n",
       "28674  0.630217 -0.793406  0.924346 -0.163191 -1.294682  0.410528 -1.177492   \n",
       "28675  1.027863  0.084394 -0.974700  0.230460  0.397149 -0.572397  0.204807   \n",
       "28676 -0.332365  0.797334  0.878293  1.357837  0.732640  0.508122  0.623108   \n",
       "28677  1.098315 -0.529875 -0.906371 -0.777389 -0.066633 -0.285113 -0.291152   \n",
       "\n",
       "             V8        V9       V10  ...       V20       V21       V22  \\\n",
       "0     -7.481774 -1.310739 -1.750813  ...  3.586292 -6.258702  1.163043   \n",
       "1     -0.185605  1.049741 -0.240491  ... -0.110982  0.038445  0.528470   \n",
       "2      0.461429  0.403048 -1.409372  ...  1.864714  0.429058 -0.520817   \n",
       "3      0.395090 -0.672139  0.292162  ...  0.049651  0.125696  0.549344   \n",
       "4     -0.622372 -1.085267  0.894765  ... -1.219154 -0.017103  1.373663   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28673 -0.286709  0.376723  0.071062  ... -0.239323 -0.341422 -0.845179   \n",
       "28674  0.340326  0.900087  0.251606  ... -0.177753  0.158912  0.972702   \n",
       "28675 -0.205689  0.352270 -0.290601  ... -0.120511 -0.438509 -1.230805   \n",
       "28676 -0.346918 -0.584673  1.218846  ...  0.039199  0.118811  0.613586   \n",
       "28677 -0.042402 -0.444799  0.822563  ... -0.147357  0.028141  0.031081   \n",
       "\n",
       "            V23       V24       V25       V26       V27       V28  Class  \n",
       "0      0.721013 -0.838813  1.973207 -1.295843 -0.047097  0.941755      0  \n",
       "1      0.078108  0.993348  0.194594  2.026379 -0.237219 -0.226298      0  \n",
       "2      2.025422  0.676446  0.019311  1.577217 -0.755467 -1.029430      0  \n",
       "3     -0.188079  0.898622 -0.414465 -0.857327  0.065929 -0.511850      0  \n",
       "4     -0.036971  0.129301 -2.831946  1.555181  1.248615  1.110322      0  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "28673  0.566116  0.117848 -0.517101  0.399455 -0.165871 -0.200830      0  \n",
       "28674 -0.056755  0.173374  0.662386 -0.018687  0.223756  0.061716      0  \n",
       "28675  0.520796  0.666969 -0.406308  0.369172 -0.150879 -0.103022      0  \n",
       "28676 -0.590491 -1.276522 -0.688309  0.210966 -2.140772 -1.059996      0  \n",
       "28677  0.337860  0.438380 -0.054341 -0.615476 -0.137767 -0.256593      0  \n",
       "\n",
       "[28678 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28678 entries, 0 to 28677\n",
      "Data columns (total 29 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      28678 non-null  float64\n",
      " 1   V2      28678 non-null  float64\n",
      " 2   V3      28678 non-null  float64\n",
      " 3   V4      28678 non-null  float64\n",
      " 4   V5      28678 non-null  float64\n",
      " 5   V6      28678 non-null  float64\n",
      " 6   V7      28678 non-null  float64\n",
      " 7   V8      28678 non-null  float64\n",
      " 8   V9      28678 non-null  float64\n",
      " 9   V10     28678 non-null  float64\n",
      " 10  V11     28678 non-null  float64\n",
      " 11  V12     28678 non-null  float64\n",
      " 12  V13     28678 non-null  float64\n",
      " 13  V14     28678 non-null  float64\n",
      " 14  V15     28678 non-null  float64\n",
      " 15  V16     28678 non-null  float64\n",
      " 16  V17     28678 non-null  float64\n",
      " 17  V18     28678 non-null  float64\n",
      " 18  V19     28678 non-null  float64\n",
      " 19  V20     28678 non-null  float64\n",
      " 20  V21     28678 non-null  float64\n",
      " 21  V22     28678 non-null  float64\n",
      " 22  V23     28678 non-null  float64\n",
      " 23  V24     28678 non-null  float64\n",
      " 24  V25     28678 non-null  float64\n",
      " 25  V26     28678 non-null  float64\n",
      " 26  V27     28678 non-null  float64\n",
      " 27  V28     28678 non-null  float64\n",
      " 28  Class   28678 non-null  int64  \n",
      "dtypes: float64(28), int64(1)\n",
      "memory usage: 6.3 MB\n"
     ]
    }
   ],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling 진행합니다.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data1.drop([\"Class\"], axis=1)\n",
    "y = data1[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21508, 28) (7170, 28) (21508,) (7170,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 예측\n",
    "y_pred = classifier.predict(X_test) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99162657e-01, 8.37342733e-04],\n",
       "       [9.99687212e-01, 3.12787618e-04],\n",
       "       [9.79470309e-01, 2.05296911e-02],\n",
       "       ...,\n",
       "       [9.99915594e-01, 8.44058313e-05],\n",
       "       [9.96014554e-01, 3.98544568e-03],\n",
       "       [9.99193219e-01, 8.06781183e-04]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0으로 분류될 확률, 1로 분류될 확률]\n",
    "classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7114,    4],\n",
       "       [  11,   41]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997907949790795"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)\n",
    "# 정확도는 0.9979로 전체 결과 중 99.79%를 정확하게 분류했습니다.(사기를 사기라고 분류하고, 사기x를 사기x라 분류하는 지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)\n",
    "# 정밀도는 91.11%로 모델이 사기라고 분류한 것 중 실제 사기였던 것의 비율은 99.15%입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884615384615384"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)\n",
    "# 재현율은 78.84%로, 실제 사기인 것 중 모델이 사기라고 분류한 것의 비율은 78.84% 였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845360824742268"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)\n",
    "# 정밀도와 재현율을 섞어서 만든 f1스코어는 0.8453이었습니다. 저는 f1스코어를 가장 중요한 평가지표로 판단할 것입니다.\n",
    "#F1 score는 지금 우리 데이터처럼 label이 불균형 구조일 때, 모형의 성능 평가가 비교적 정확하기 때문에 f1스코어가 가장 중요합니다.\n",
    "#(현재 우리 데이터에는 사기가 아닌 0의 데이터가 사기인 데이터 1보다 100배정도 많은 매우 불균형한 상태입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8939497914280155\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAV9ElEQVR4nO3db2xcV5nH8d8TO388bprYYzekTmYcsmlLoC1tTQuIQltUmhRQlxXStkWgrUBRdynal61WWnjBGxD7gkUUoqjqVmhX5AVUkF0FKqQVFKnbpalUSv+oKFsSx00gjp1/9djxv2dfzJ3xvZPrzE089viMvx/JkmfmeOacJv319vF57jF3FwAgfKuaPQEAQGMQ6ADQIgh0AGgRBDoAtAgCHQBaRHuzPrinp8f7+/ub9fEAEKSXX375lLv3pr3WtEDv7+/XoUOHmvXxABAkMzs632uUXACgRRDoANAiCHQAaBEEOgC0CAIdAFpE3UA3s6fN7KSZvTbP62Zm3zOzw2b2qpnd2vhpAgDqyXKF/oykXZd4fbekHdHXHkk/XPi0AACXq+4+dHd/3sz6LzHkAUk/8vJ9eF80s41mttndTzRqkgAQsrOlKR0dHdPRkZIGR0u6acsG3bkjtTdoQRrRWNQn6Vjs8VD03EWBbmZ7VL6KV6FQaMBHA0Dzzc66Tp6/oKMjYzo6WtLgSElHR0vlxyMlnR2fSoz/+7u2L9tAt5TnUk/NcPd9kvZJ0sDAACdrAAjG1Mys3jk9ngjq8hX3mAZHS5qYmq2ObVtl6tvYoWI+p8/evFnF7k4V8jkV8zkVunPKrVmcJv1GvOuQpK2xx1skHW/A+wLAkipNTieC+shI5Wp7TMfPTGhmdu46dN3qVSp2d6qY79THd/Sq2NOpYnc5tK/d2KHVbUu/ibARgX5A0mNmtl/SHZLOUj8HsBy5u06XpnRkZKwc1FFYD46UdGSkpFPvXkiM35hbrWK+U7ds7dJff7B8dV3Md6o/n1Pv+rUySytQNE/dQDezH0u6S1KPmQ1J+oak1ZLk7nslHZR0v6TDkkqSHlmsyQJAPbOzrhPnJnR0ZC6oByu/kBwp6fyF6cT4zRvWqdCd0z039KqY71Qxn6uWSDZ0rG7SKq5Mll0uD9V53SV9tWEzAoA6LkzP6NjoeDWoK7tHjoyMaWh0XJMzc/Xs1W2mrV05FfI5DRS7VMiXSyP9PTlt6cpp3eq2Jq6ksZp2+1wAuJTzE1PVsK6URSrBffzsuDy2raJzTZsK+U5dv2m97t25Kaptl0sk127sUNuq5VUaWSwEOoCmcHcNv3shVsue2z0yOFrS6NhkYnzPVWtU6M7p9m3dKkRX2IUouPOda5ZdPbsZCHQAi2Z6ZlYnzk7oSCyo46Fdmpypjl1l0uYNHervyem+978nqmXnou1+nbpqLXFVD/+EACzIxNRMFNSx/dmjJQ2OjGno9LimY1v91rSvKu8U6c7pI9vz6s9H+7O7y/XsNe3cL3AhCHQAdZ0pTSaCOl7b/su55Fa/9eva1Z/v1Pv7Nuj+GzdHtexO9ffktGn9Oq1aIfXsZiDQASRb16OgniuRXNy6fs36tSrmc/rYX/WqPz9XFil257Qxt5p6dpMQ6MAKMTUzq6HT4+X92aMlHTkV2589WtKF6WTr+pauDhW651rXi1FoF7pz6ljTOlv9WgmBDrSQsQvT1bb1uRJJeX/28TPjipWz1bG6TYXunLb1dOqu63ur+7Ob2bqOhSHQgYC4u0bHJpNb/Kpb/i5uXe/KrVYh36lbC1363C19sU7I5dm6joUh0IFlZmbWdeLseCKo41v93o21rptJm69ep0I+p0/ecE31jn6V3SNXrwurdR0LQ6ADTVBpXT9auz97tHTJ1vVKU00xCu5Wa13HwhDowCI5NzFV7YKs3t0vamE/cW4i0bp+1dp2Fbpzidb1yu6RzRtWTus6FoZAB66Qu2v4/IVqWWQwusI+En1/upTc6ldpXf/we/Oxww5oXUfjEOjAJUzPzOr4mQkdrR52MFciSWtdvzY6pWbXBzarPxbahXyO1nUsOv6GYcUbn5yp1rArjTRHou/fSWldr2zt++j2nnJgR7+E7NvYQes6mopAx4pwpjSpI9FukcGa/dknzye3+l29rl3FfKc+0LdBn7kpeR4kretYzgh0tITZWddfzk9U92UfiZ++PjKmcxPJU2o2Xb1Wxe5Offy63uod/fqjPdobc2uatApgYQh0BGNyelZDp0uxoJ47yPdYTet6+ypTX9S6/sGtfdXDDmhdRysj0LGsVFrXK3uy423saa3rxXxO7+3p1N3XJ8+DvHbjOrXTuo4VhkDHknJ3jYxNJu83MjL3C8lT7yZPqemKTl2/rdilv7mlr3y/kaie3XsVretAHIGOhqu0rs93HuT8reubVOzJzZ0HSes6cFkIdFyRiamZcj17ZK6RplLbPna6pKmZudrI6jbT1uiUmmTreqe2dHXQug40CIGOeZ0dn6q2q8d3jwyOlvTneVrXb9i8Xp+qOQ+S1nVgaRDoK1i8df3IqbmmmsoxYxe3rpdPqflI1LoePw+ym9Z1oOkI9BY3PTOrd86MJ4L6SHS1PTha0vjUxa3r/flO7b5xc7UjktZ1IAz8G9oCSpPT1avrRIlktKSh0+Oaie31W1s5dT2f08d29CT2Z9O6DoSNQA+Au+tMaSpxSk18219a63p/T6dujLWuV34Jec36tbSuAy2KQF8mZmddfz43cdF5kJUAPz9P6/onruuNtvjNnQdJ6zqwMhHoS6jauh7rhKzeKGq0pMma1vUtXR0q5Dt1y9au6hV2MZ/T1i5a1wFcjEBvsHcvTCfu6BcvkZw4m966vr23U/fccM3c/mxa1wFcAQL9Ms21rsdr2XPBPTKWbF3v7iyfUjPQ36VifsvczhFa1wE0WKZAN7Ndkv5VUpukp9z9WzWvb5D075IK0Xv+i7v/W4PnumRmZl3Hz4xrcDR2FmRs299Y7JQaM+naDeW7+t27c1O0L5vWdQBLr26gm1mbpCcl3StpSNJLZnbA3d+IDfuqpDfc/bNm1ivpLTP7D3efTHnLZWFiakbHahppjlS3+iVb19e0rdKW7g4Vu3O6Y1t39eZQhe5Obe3u0Np26tkAmi/LFfrtkg67+9uSZGb7JT0gKR7oLmm9lesHV0kalTRd+0ZLrdK6fqR6tNjc/uwTZycSY9evbVchn9P7Nq/XfZXW9egXke+5eh2t6wCWvSyB3ifpWOzxkKQ7asZ8X9IBScclrZf0t+4+WzNGZrZH0h5JKhQKVzLfBHfXyfMX5naNxK62j46WdGa+1vXt+URZhNZ1AK0gS6CnpZzXPL5P0iuS7pG0XdKvzOy37n4u8UPu+yTtk6SBgYHa98jkwvSMHv/Jq3rjxDkNjpY0MTX3341VJvV1dajY3an7byyful6oBHd3Tp20rgNoYVkSbkjS1tjjLSpficc9Iulb7u6SDpvZnyTdIOl3DZllzOBIST975bg+uHWjvnBHsRrW/flO9XV1aDVb/QCsUFkC/SVJO8xsm6R3JD0o6eGaMYOSPinpt2a2SdL1kt5u5ERrfeXObfrMTdcu5kcAQFDqBrq7T5vZY5KeU3nb4tPu/rqZPRq9vlfSNyU9Y2Z/ULlE87i7n1rEeQMAamQqKrv7QUkHa57bG/v+uKRPNXZq88xlKT4EAAJEwRkAWkSwgW6pm28AYOUKNtABAEnBBbpTRAeAVMEFegVNnQCQFGygAwCSggt0Z+MiAKQKLtABAOmCDXRK6ACQFGygAwCSggt0ti0CQLrgAr2CbYsAkBRsoAMAkoILdEouAJAuuEAHAKQLONApogNAXMCBDgCICy7Qaf0HgHTBBXoF2xYBICnYQAcAJAUX6GxbBIB0wQU6ACBdsIFOCR0AkoINdABAEoEOAC2CQAeAFhFsoBsb0QEgIdhABwAkBRfo7EMHgHTBBXoFBRcASMoU6Ga2y8zeMrPDZvbEPGPuMrNXzOx1M/tNY6cJAKinvd4AM2uT9KSkeyUNSXrJzA64+xuxMRsl/UDSLncfNLNrFmm+3G0RAOaR5Qr9dkmH3f1td5+UtF/SAzVjHpb0rLsPSpK7n2zsNAEA9WQJ9D5Jx2KPh6Ln4q6T1GVmvzazl83sS2lvZGZ7zOyQmR0aHh6+shlX32tBPw4ALSdLoKdFZ23do13SbZI+Lek+Sf9sZtdd9EPu+9x9wN0Hent7L3uyAID51a2hq3xFvjX2eIuk4yljTrn7mKQxM3te0s2S/tiQWcawbREA0mW5Qn9J0g4z22ZmayQ9KOlAzZifS7rTzNrNLCfpDklvNnaqSZRcACCp7hW6u0+b2WOSnpPUJulpd3/dzB6NXt/r7m+a2S8lvSppVtJT7v7aYk4cAJCUpeQidz8o6WDNc3trHn9H0ncaN7V55rLYHwAAgQq2UxQAkBRsoBvN/wCQEGygAwCSggt0Z98iAKQKLtCrqLgAQEK4gQ4ASAgu0Cm4AEC64AIdAJAu2ECnhA4AScEGOgAgKbhAZ9ciAKQLLtArjNstAkBCsIEOAEgi0AGgRQQY6BTRASBNgIFeRgUdAJKCDXQAQFJwgc62RQBIF1ygV7BrEQCSgg10AEASgQ4ALSK4QKeEDgDpggv0Cg6JBoCkYAMdAJAUXKCzbREA0gUX6ACAdMEGOvvQASAp2EAHACQFF+hOER0AUgUX6BVUXAAgKdhABwAkZQp0M9tlZm+Z2WEze+IS4z5kZjNm9vnGTTGJggsApKsb6GbWJulJSbsl7ZT0kJntnGfctyU91+hJAgDqy3KFfrukw+7+trtPStov6YGUcV+T9FNJJxs4v/lRRAeAhCyB3ifpWOzxUPRclZn1SfqcpL2XeiMz22Nmh8zs0PDw8OXOFQBwCVkCPe1auLaU/V1Jj7v7zKXeyN33ufuAuw/09vZmnGLte1zRjwFAy2vPMGZI0tbY4y2SjteMGZC038rtmz2S7jezaXf/WSMmmYa7LQJAUpZAf0nSDjPbJukdSQ9Kejg+wN23Vb43s2ck/ddihjkA4GJ1A93dp83sMZV3r7RJetrdXzezR6PXL1k3bzRn4yIApMpyhS53PyjpYM1zqUHu7n+38GkBAC5XsJ2i3G0RAJKCDXQAQFJ4gU4JHQBShRfoESouAJAUbKADAJIIdABoEcEFOiV0AEgXXKBXGPsWASAh2EAHACQFF+jcbREA0gUX6BVUXAAgKdhABwAkEegA0CKCC3RunwsA6YIL9ApK6ACQFGygAwCSggt0ti0CQLrgAr2CbYsAkBRsoAMAkgh0AGgRwQU6JXQASBdcoM+hiA4AcQEHOgAgLrhAd/YtAkCq4AIdAJAu2EBnHzoAJAUb6ACApOACnQo6AKQLLtArqLgAQFKwgQ4ASMoU6Ga2y8zeMrPDZvZEyutfMLNXo68XzOzmxk81Qs0FAFLVDXQza5P0pKTdknZKesjMdtYM+5OkT7j7TZK+KWlfoycKALi0LFfot0s67O5vu/ukpP2SHogPcPcX3P109PBFSVsaO82LGfsWASAhS6D3SToWezwUPTefL0v6RdoLZrbHzA6Z2aHh4eHsswQA1JUl0NMuhVMr2WZ2t8qB/nja6+6+z90H3H2gt7c3+ywTH0wRHQDStGcYMyRpa+zxFknHaweZ2U2SnpK0291HGjO9+VFwAYCkLFfoL0naYWbbzGyNpAclHYgPMLOCpGclfdHd/9j4aQIA6ql7he7u02b2mKTnJLVJetrdXzezR6PX90r6uqS8pB9Ev6ycdveBxZs2AKBWlpKL3P2gpIM1z+2Nff8VSV9p7NTmm8tSfAoAhCfYTlF2LQJAUrCBDgBICi7QKbkAQLrgAr3C2LgIAAnBBjoAIIlAB4AWEVygU0IHgHTBBXoF2xYBICnYQAcAJAUX6M6+RQBIFVygAwDSEegA0CIIdABoEcEFOhV0AEgXXKBXsG0RAJKCDXQAQFJwgc6uRQBIF1ygV3C3RQBICjbQAQBJBDoAtIgAA50iOgCkCTDQy9i2CABJwQY6ACApuEBn2yIApAsu0AEA6YINdGroAJAUbKADAJKCC3RK6ACQLrhAr6D1HwCSgg10AEASgQ4ALSJToJvZLjN7y8wOm9kTKa+bmX0vev1VM7u18VMtYx86AKSrG+hm1ibpSUm7Je2U9JCZ7awZtlvSjuhrj6QfNnieKfNa7E8AgLBkuUK/XdJhd3/b3Scl7Zf0QM2YByT9yMtelLTRzDY3eK4AgEvIEuh9ko7FHg9Fz13uGJnZHjM7ZGaHhoeHL3eukqT3bFinT9+4WVetbb+inweAVpUlFdOKG7WV7Cxj5O77JO2TpIGBgSuqht9W7NJtxa4r+VEAaGlZrtCHJG2NPd4i6fgVjAEALKIsgf6SpB1mts3M1kh6UNKBmjEHJH0p2u3yYUln3f1Eg+cKALiEuiUXd582s8ckPSepTdLT7v66mT0avb5X0kFJ90s6LKkk6ZHFmzIAIE2m3yy6+0GVQzv+3N7Y9y7pq42dGgDgctApCgAtgkAHgBZBoANAiyDQAaBFmDfpbldmNizp6BX+eI+kUw2cTghY88rAmleGhay56O69aS80LdAXwswOuftAs+exlFjzysCaV4bFWjMlFwBoEQQ6ALSIUAN9X7Mn0ASseWVgzSvDoqw5yBo6AOBioV6hAwBqEOgA0CKWdaAvp8Opl0qGNX8hWuurZvaCmd3cjHk2Ur01x8Z9yMxmzOzzSzm/xZBlzWZ2l5m9Ymavm9lvlnqOjZbh7/YGM/tPM/t9tOag79pqZk+b2Ukze22e1xufX+6+LL9UvlXv/0l6r6Q1kn4vaWfNmPsl/ULlE5M+LOl/mz3vJVjzRyV1Rd/vXglrjo37b5Xv+vn5Zs97Cf6cN0p6Q1IhenxNs+e9BGv+J0nfjr7vlTQqaU2z576ANX9c0q2SXpvn9Ybn13K+Ql+Jh1PXXbO7v+Dup6OHL6p8OlTIsvw5S9LXJP1U0smlnNwiybLmhyU96+6DkuTuoa87y5pd0nozM0lXqRzo00s7zcZx9+dVXsN8Gp5fyznQG3Y4dUAudz1fVvm/8CGru2Yz65P0OUl71Rqy/DlfJ6nLzH5tZi+b2ZeWbHaLI8uavy/pfSofX/kHSf/o7rNLM72maHh+ZTrgokkadjh1QDKvx8zuVjnQP7aoM1p8Wdb8XUmPu/tM+eIteFnW3C7pNkmflNQh6X/M7EV3/+NiT26RZFnzfZJekXSPpO2SfmVmv3X3c4s8t2ZpeH4t50BfiYdTZ1qPmd0k6SlJu919ZInmtliyrHlA0v4ozHsk3W9m0+7+syWZYeNl/bt9yt3HJI2Z2fOSbpYUaqBnWfMjkr7l5QLzYTP7k6QbJP1uaaa45BqeX8u55LISD6euu2YzK0h6VtIXA75ai6u7Znff5u797t4v6SeS/iHgMJey/d3+uaQ7zazdzHKS7pD05hLPs5GyrHlQ5f8jkZltknS9pLeXdJZLq+H5tWyv0H0FHk6dcc1fl5SX9IPoinXaA75TXcY1t5Qsa3b3N83sl5JelTQr6Sl3T93+FoKMf87flPSMmf1B5XLE4+4e7G11zezHku6S1GNmQ5K+IWm1tHj5Res/ALSI5VxyAQBcBgIdAFoEgQ4ALYJAB4AWQaADQIsg0AGgRRDoANAi/h8hvkafBiL+6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn에서 ROC 패키지 활용\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred,pos_label = 1)\n",
    "\n",
    "# ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "# AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "# 사기가 아닌데 사기라고 잘못 판단한 FPR과 sensitivity를 고려한 그래프에서 AUC는 0.89가 나왔습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성능개선- 경사하강법 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z\n",
    "\n",
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1/(1+np.exp(-z))   \n",
    "    return p\n",
    "\n",
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = -(y*np.log(p)+(1-y)*np.log(1-p))\n",
    "    return loss\n",
    "\n",
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = 0.5*((y-y_hat)**2)\n",
    "    return loss\n",
    "\n",
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = loss/n #loss 평균값으로 계산\n",
    "    return loss\n",
    "\n",
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = -(y-y_hat)*X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = -(y-p)*X[j]\n",
    "    return gradient\n",
    "\n",
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list\n",
    "\n",
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= (learning_rate/n)\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters\n",
    "\n",
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, len(X_batch))\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, len(X_batch))\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.27628720251613836  params: [-1.12384614e+00  9.18383514e-01 -2.27400178e+00  1.11371685e+00\n",
      " -1.27637130e+00 -3.43542826e-01 -2.38926138e+00  5.88560151e-01\n",
      " -8.12577409e-01 -2.40917927e+00  1.45361469e+00 -2.82715954e+00\n",
      "  1.18249461e-02 -2.99995280e+00 -8.06222945e-02 -2.28776829e+00\n",
      " -4.72946913e+00 -1.28774604e+00  4.33979723e-01  2.12408460e-01\n",
      "  2.60186304e-01  7.80761183e-02  1.36240228e-02 -2.13338224e-02\n",
      " -1.99114949e-03  3.90943113e-02  1.13968142e-01  4.84974955e-02]  gradients: [0.010038540977724303, -0.009736729491332587, -0.0064750785836211276, 0.000935699957362486, 0.01033433962575463, 0.012163026420000743, -0.0050044882812180095, 0.004289712872220973, -0.00889197682634615, 0.013759688781228073, -0.01107366327738835, -0.0025513004377216426, -0.011727488435409337, 0.0032361328605812895, -0.013838363203430681, -0.007281318540256558, -0.003234646696507862, 0.004372482321578326, -0.006885021301309882, -0.0018008639202644547, -0.0012250138838183725, -0.0005199411802852176, -0.00037318393641827237, 0.01439088094183044, 0.004261901540790424, -0.003330479431652487, 0.0009657802786309796, -0.0006570986408804919]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.74289835,  1.39950346, -3.36032579,  1.66598479, -1.94176151,\n",
       "       -0.58220466, -3.57260806,  0.77910385, -1.36616058, -3.57616786,\n",
       "        2.15585   , -4.29379121,  0.08913455, -4.44404488, -0.04630526,\n",
       "       -3.30449504, -7.1262059 , -1.97899309,  0.6419144 ,  0.31658215,\n",
       "        0.41530857,  0.03017929, -0.04187527, -0.01189784,  0.11043605,\n",
       "       -0.00902781,  0.24032032,  0.10980443])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적화 과제를 통해 경사하강법을 배웠으니, 최적의 모수를 찾아주기 위해 경사하강법을 진행하여, 새로운 parameter들을 얻습니다.\n",
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 10)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 예측값을 얻어 성능평가 지표들을 확인해봅니다.\n",
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7118,    0],\n",
       "       [   5,   47]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.999302649930265\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)\n",
    "# 정확도는 기존 0.9979에서 0.9993으로 미세하게 증가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_predict)\n",
    "# 정밀도는 기존 0.9111에서 1로, 최고점에 도달했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038461538461539"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_predict)\n",
    "# 재현율은 기존 0.7884에서 0.9로 개선되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9494949494949495"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict)\n",
    "# 가장 중요하게 생각하는 f1 스코어는 기존 0.8453에서 0.9494로 개선되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9519230769230769\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, y_predict,pos_label = 1)\n",
    "\n",
    "# ROC curve\n",
    "plt.plot(fpr2,tpr2)\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "# AUC\n",
    "auc = np.trapz(tpr2,fpr2)\n",
    "print(\"AUC:\",auc)\n",
    "# AUC는 기존0.8939에서 0.95로 개선되었습니다. (기존 주황색에서 파란색으로 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff 포인트를 변화를 주면서 구체적으로 어떤 수치에서 cutoff를 정해야 f1스코어가 가장 높아지는지 보겠습니다.\n",
    "P_1 = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    P_1.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_2=np.array(P_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7170,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.01 민감도 :  1.00 정밀도 :  0.01 cut off :  0.01 f1 :  0.01\n",
      "정확도 :  0.02 민감도 :  1.00 정밀도 :  0.01 cut off :  0.12 f1 :  0.01\n",
      "정확도 :  0.99 민감도 :  0.96 정밀도 :  0.46 cut off :  0.23 f1 :  0.62\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.34 f1 :  0.95\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.45 f1 :  0.95\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.55 f1 :  0.95\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.66 f1 :  0.95\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.77 f1 :  0.95\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.88 f1 :  0.95\n",
      "정확도 :  1.00 민감도 :  0.90 정밀도 :  1.00 cut off :  0.99 f1 :  0.95\n"
     ]
    }
   ],
   "source": [
    "Cut_off=np.linspace(0.01,0.99,10) #cut off 값 만들기\n",
    "for cutoff in Cut_off:\n",
    "    y_pred2=np.where(P_2.reshape(-1)>=cutoff,1,0)\n",
    "    acc=accuracy_score(y_true=y_test,y_pred=y_pred2) #정확도\n",
    "    recall=recall_score(y_true=y_test,y_pred=y_pred2) #민감도\n",
    "    precision=precision_score(y_true=y_test,y_pred=y_pred2) #정밀도\n",
    "    f1=f1_score(y_test,y_pred2) # f1 score\n",
    "\n",
    "    print(f\"정확도 : {acc : 0.2f}\",f\"민감도 : {recall : 0.2f}\",f\"정밀도 : {precision : 0.2f}\",f\"cut off : {cutoff : 0.2f}\",f\"f1 : {f1 : 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.34이상의 cutoff부터 f1스코어가 0.95에 도달하는 것을 알 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression_과제3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
